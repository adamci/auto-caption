{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# CRNN from scratch using TensorFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this notebook is to take what I've learned from the [Udacity Deep Learning course](https://www.udacity.com/course/deep-learning--ud730) and apply it to training an algorithm that can perform image to text translation on the [Stanford Street View House Numbers (SVHN) Dataset](http://ufldl.stanford.edu/housenumbers/), successfully recognizing street address numbers in real-world images.\n",
    "\n",
    "Because I've recently gotten very excited about LSTMs, I'll be attempting this task using a [CRNN](https://arxiv.org/pdf/1507.05717.pdf), connecting the output of a convolutional net to an RNN decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import scipy.io\n",
    "import gzip\n",
    "import tarfile\n",
    "import h5py\n",
    "from PIL import Image\n",
    "import six.moves.cPickle as pickle\n",
    "from six.moves import urllib\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many thanks to Imad Ali who gave a [great example](https://github.com/imadmali/svhn-format1) of how to preprocess this dataset using Python's `h5py` library. The next few cells are adapted from his work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/train.tar.gz\n",
      "../data/test.tar.gz\n"
     ]
    }
   ],
   "source": [
    "data_dir = '../data'\n",
    "\n",
    "# Download the SVHN dataset if it is not present\n",
    "def check_dataset(dataset):\n",
    "    # Check if dataset is in the data directory.\n",
    "    new_path = os.path.join(data_dir, dataset)\n",
    "    if (not os.path.isdir(data_dir)):\n",
    "        os.mkdir(data_dir)\n",
    "    if (not os.path.isfile(new_path)):\n",
    "        origin = ('http://ufldl.stanford.edu/housenumbers/' + dataset)\n",
    "        print('Downloading data from %s' % origin)\n",
    "        urllib.request.urlretrieve(origin, new_path)\n",
    "    return new_path\n",
    "\n",
    "print(check_dataset('train.tar.gz'))\n",
    "print(check_dataset('test.tar.gz'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_channels = 3\n",
    "\n",
    "def format_data(dataset):\n",
    "    print '... processing data (should only occur when downloading for the first time)'\n",
    "    # Unzip data from scratch\n",
    "    data_type = os.path.splitext(os.path.splitext(dataset)[0])[0]\n",
    "    unzipped_path = os.path.join(data_dir, data_type)\n",
    "    if os.path.exists(unzipped_path):\n",
    "        shutil.rmtree(unzipped_path)\n",
    "    tar = tarfile.open(os.path.join(data_dir, dataset), 'r:gz')\n",
    "    tar.extractall(data_dir)\n",
    "\n",
    "    # Access label information in digitStruct.mat\n",
    "    f = h5py.File(os.path.join(data_dir, data_type, 'digitStruct.mat'), 'r')\n",
    "    digit_struct_name = f['digitStruct']['name']\n",
    "    digit_struct_bbox = f['digitStruct']['bbox']\n",
    "\n",
    "    def getName(n):\n",
    "        return ''.join([chr(c[0]) for c in f[digit_struct_name[n][0]].value])\n",
    "\n",
    "    def bboxHelper(attr):\n",
    "        if (len(attr) > 1):\n",
    "            attr = [f[attr.value[j].item()].value[0][0] for j in range(len(attr))]\n",
    "        else:\n",
    "            attr = [attr.value[0][0]]\n",
    "        return attr\n",
    "\n",
    "    def getBbox(n):\n",
    "        bbox = {}\n",
    "        bb = digit_struct_bbox[n].item()\n",
    "        bbox['height'] = bboxHelper(f[bb][\"height\"])\n",
    "        bbox['label'] = bboxHelper(f[bb][\"label\"])\n",
    "        bbox['left'] = bboxHelper(f[bb][\"left\"])\n",
    "        bbox['top'] = bboxHelper(f[bb][\"top\"])\n",
    "        bbox['width'] = bboxHelper(f[bb][\"width\"])\n",
    "        return bbox\n",
    "\n",
    "    # Process bbox\n",
    "    print '... creating image box bound dict for %s data' % data_type\n",
    "    image_dict = {}\n",
    "    for i in range(len(digit_struct_name)):\n",
    "        image_dict[getName(i)] = getBbox(i)\n",
    "        if (i % 2000 == 0):\n",
    "            print '\\timage dict processing: %i/%i complete' %(i, len(digit_struct_name))\n",
    "    print '... dict processing complete'\n",
    "\n",
    "    # Process the data\n",
    "    print('... processing image data and labels')\n",
    "    names = [item for item in os.listdir(unzipped_path) if item.endswith('.png')]\n",
    "\n",
    "    x, y = [], []\n",
    "    for i in range(len(names)):\n",
    "        y.append(image_dict[names[i]]['label'])\n",
    "        image = Image.open(unzipped_path + '/' + names[i])\n",
    "        # Calculate the total bounding box from the exterior corners of all bounding boxes\n",
    "        left = int(min(image_dict[names[i]]['left']))\n",
    "        upper = int(min(image_dict[names[i]]['top']))\n",
    "        right = int(max(image_dict[names[i]]['left'])) + int(max(image_dict[names[i]]['width']))\n",
    "        lower = int(max(image_dict[names[i]]['top'])) + int(max(image_dict[names[i]]['height']))\n",
    "        image = image.crop(box = (left, upper, right, lower))\n",
    "        image = image.resize([image_size, image_size])\n",
    "        image_array = np.array(image)\n",
    "        # Normalize mean and standard deviation\n",
    "        image_array = (image_array - 255. / 2) / 255.\n",
    "        x.append(image_array)\n",
    "        if (i % 2000 == 0):\n",
    "            print '\\timage processing: %i/%i complete' %(i,len(names))\n",
    "    print '... image processing complete'\n",
    "\n",
    "    # Save data\n",
    "    print '... pickling data'\n",
    "    out = {'labels': y, 'images': x}\n",
    "    out_path = os.path.join(data_dir, data_type + 'pkl.gz')\n",
    "    p = gzip.open(out_path, 'wb')\n",
    "    pickle.dump(out, p)\n",
    "    p.close()\n",
    "\n",
    "    tar.close()\n",
    "    \n",
    "    # clean up (delete test/train folders that were used to create the pickled data)\n",
    "    shutil.rmtree(unzipped_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only format the data if the processed pickle files don't exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if (not os.path.isfile(os.path.join(data_dir, 'trainpkl.gz'))):\n",
    "    format_data('train.tar.gz')\n",
    "\n",
    "f_train = gzip.open(os.path.join(data_dir, 'trainpkl.gz'), 'rb')\n",
    "train_data = pickle.load(f_train)\n",
    "f_train.close()\n",
    "\n",
    "if (not os.path.isfile(os.path.join(data_dir, 'testpkl.gz'))):\n",
    "    format_data('test.tar.gz')\n",
    "\n",
    "f_test = gzip.open(os.path.join(data_dir, 'testpkl.gz'), 'rb')\n",
    "test_data = pickle.load(f_test)\n",
    "f_test.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_length = 6\n",
    "\n",
    "# Convert data format\n",
    "def convert_data_format(data):\n",
    "    images = data.pop('images')\n",
    "    X = np.array(images)\n",
    "\n",
    "    labels = data.pop('labels')\n",
    "    for i in range(len(labels)):\n",
    "        l = len(labels[i])-1\n",
    "        zeros = np.zeros(label_length - l - 1).tolist()\n",
    "        labels[i].extend(zeros)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    return (X,y)\n",
    "\n",
    "train_set = convert_data_format(train_data)\n",
    "train_set_len = len(train_set[1])\n",
    "\n",
    "# Extract validation dataset from train dataset (10% of the train_set)\n",
    "valid_set = [x[-(train_set_len//10):] for x in train_set]\n",
    "train_set = [x[:-(train_set_len//10)] for x in train_set]\n",
    "test_set = convert_data_format(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_set`, `valid_set`, and `test_set` each contain a list `['images', 'labels']`.\n",
    "* `'images'` is a 4D numpy array of dimensions num_images x height x width x channels (RGB)\n",
    "* `'labels'` is a 2D numpy array where each row is a house number and each element of that row is a digit. The rows\n",
    "  are padded at a length of 6 digits where the number `0` represents the padding, and the number `10` represents the\n",
    "  digit `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((30062, 32, 32, 3), (30062, 6))\n",
      "((3340, 32, 32, 3), (3340, 6))\n",
      "((13068, 32, 32, 3), (13068, 6))\n"
     ]
    }
   ],
   "source": [
    "print(train_set[0].shape, train_set[1].shape)\n",
    "print(valid_set[0].shape, valid_set[1].shape)\n",
    "print(test_set[0].shape, test_set[1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's display an image from the test set. The image will look odd because of the mean and standard deviation normalization we did in the `format_data()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  2.  10.  10.   0.   0.   0.]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nipxfiIQi5xciocj5hUgo/x+IH4cjW0CCJgAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f9ccea28790>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "data_index = 994\n",
    "plt.imshow(test_set[0][data_index])\n",
    "print(test_set[1][data_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From these datasets, we need to build batches that can be read by convolutional and LSTM layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "vocabulary_size = 12 # buffer, digits 1-9, 0 digit, and a GO token for the lstm, in that order\n",
    "\n",
    "class BatchGenerator(object):\n",
    "  def __init__(self, data_set, batch_size):\n",
    "    self._images = data_set[0]\n",
    "    self._labels = data_set[1]\n",
    "    self._dataset_size = data_set[1].shape[0]\n",
    "    self._label_size = data_set[1].shape[1]\n",
    "    self._batch_size = batch_size\n",
    "    self._cursor = 0\n",
    "  \n",
    "  def next(self):\n",
    "    \"\"\"Generate a single batch from the current cursor position in the data.\"\"\"\n",
    "    image_batch = []\n",
    "    for b in range(self._batch_size):\n",
    "        cursor = self._cursor + b % self._dataset_size\n",
    "        image_batch.append(self._images[cursor])\n",
    "    label_batch = []\n",
    "    for i in range(self._label_size):\n",
    "        batch = np.zeros(shape=(self._batch_size, vocabulary_size), dtype=np.float64)\n",
    "        for b in range(self._batch_size):\n",
    "            cursor = self._cursor + b % self._dataset_size\n",
    "            batch[b, int(self._labels[cursor][i])] = 1.0\n",
    "        label_batch.append(batch)\n",
    "    self._cursor = self._cursor + self._label_size % self._dataset_size\n",
    "    return [np.array(image_batch), label_batch]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((16, 32, 32, 3), 6, (16, 12))\n"
     ]
    }
   ],
   "source": [
    "test_batches = BatchGenerator(test_set, batch_size)\n",
    "batch = test_batches.next()\n",
    "print(batch[0].shape, len(batch[1]), batch[1][0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's build the model: first, the convolutional layer, and then the (slightly simplified) LSTM cell.\n",
    "Notice that the output of an LSTM cell is used as the input of the subsequent cell intead of using the correct label as the input, a competing architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_patch_size = 5\n",
    "conv_depth = 16\n",
    "conv_num_hidden = 64\n",
    "lstm_num_nodes = 64\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "    # Input data.\n",
    "    train_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    valid_dataset = tf.placeholder(\n",
    "        tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    test_dataset = tf.constant(test_set[0])\n",
    "    go_token = tf.constant(\n",
    "        np.c_[np.zeros((batch_size, vocabulary_size-1)), np.ones(batch_size)], dtype=tf.float32)\n",
    "\n",
    "    # Label data to be used as outputs\n",
    "    train_labels = list()\n",
    "    for _ in range(label_length):\n",
    "        train_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    valid_labels = list()\n",
    "    for _ in range(label_length):\n",
    "        valid_labels.append(tf.placeholder(tf.float32, shape=[batch_size,vocabulary_size]))\n",
    "    \n",
    "    # CNN variables\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "        [conv_patch_size, conv_patch_size, num_channels, conv_depth], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([conv_depth]))\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "        [conv_patch_size, conv_patch_size, conv_depth, conv_depth], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[conv_depth]))\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "        [image_size // 4 * image_size // 4 * conv_depth, conv_num_hidden], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[conv_num_hidden]))\n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "        [conv_num_hidden, lstm_num_nodes], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[lstm_num_nodes]))\n",
    "\n",
    "    # LSTM variables\n",
    "    # Combined input, output, and bias matrices of the four gates [input, forget, memory, output]\n",
    "    combined_i = tf.Variable(tf.truncated_normal([vocabulary_size, lstm_num_nodes*4], -0.1, 0.1))\n",
    "    combined_o = tf.Variable(tf.truncated_normal([lstm_num_nodes, lstm_num_nodes*4], -0.1, 0.1))\n",
    "    combined_b = tf.Variable(tf.zeros([1, lstm_num_nodes * 4]))\n",
    "    # Variable saving state across unrollings.\n",
    "    saved_output = tf.Variable(tf.zeros([batch_size, lstm_num_nodes]), trainable=False)\n",
    "    # Classifier weights and biases.\n",
    "    w = tf.Variable(tf.truncated_normal([lstm_num_nodes, vocabulary_size], -0.1, 0.1))\n",
    "    b = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "    \n",
    "    # Models\n",
    "    def cnn_model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer1_biases)\n",
    "        pooling = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        conv = tf.nn.conv2d(pooling, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(conv + layer2_biases)\n",
    "        pooling = tf.nn.max_pool(hidden, [1, 2, 2, 1], [1, 2, 2, 1], padding='SAME')\n",
    "        shape = pooling.get_shape().as_list()\n",
    "        reshape = tf.reshape(pooling, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.matmul(reshape, layer3_weights) + layer3_biases)\n",
    "        return tf.matmul(hidden, layer4_weights) + layer4_biases\n",
    "\n",
    "    def lstm_cell(i, o, state):\n",
    "        \"\"\"Similar to http://arxiv.org/pdf/1402.1128v1.pdf without various connections\n",
    "        between the previous state and the gates.\"\"\"\n",
    "        input, forget, memory, output = tf.split(\n",
    "            tf.matmul(i, combined_i) + tf.matmul(o, combined_o) + combined_b,\n",
    "            num_or_size_splits=4, axis=1)\n",
    "        input_gate  = tf.sigmoid(input)\n",
    "        forget_gate = tf.sigmoid(forget)\n",
    "        output_gate = tf.sigmoid(output)\n",
    "        state = forget_gate * state + input_gate * tf.tanh(memory)\n",
    "        return output_gate * tf.tanh(state), state\n",
    "    \n",
    "    def crnn_model(input_data):\n",
    "        # Convolutional step\n",
    "        conv_state = cnn_model(input_data)\n",
    "\n",
    "        # Unrolled LSTM loop.\n",
    "        outputs = list()\n",
    "        output = saved_output\n",
    "        state = conv_state\n",
    "        input = go_token\n",
    "        for _ in range(label_length):\n",
    "            output, state = lstm_cell(input, output, state)\n",
    "            outputs.append(output)\n",
    "            # Feed output back to input instead of using corrective inputs\n",
    "            input = tf.nn.xw_plus_b(output, w, b)\n",
    "            # input = sample(tf.nn.softmax(tf.nn.xw_plus_b(sample_output, w, b))) # do we 1-hot inputs?\n",
    "\n",
    "        # Classifier.\n",
    "        logits = tf.nn.xw_plus_b(tf.concat(outputs, 0), w, b)\n",
    "        loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=tf.concat(train_labels, 0), logits=logits))\n",
    "        \n",
    "        return logits, loss\n",
    "    \n",
    "    # Training loop\n",
    "    train_logits, train_loss = crnn_model(train_dataset)\n",
    "\n",
    "    # Optimizer.\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(10.0, global_step, 5000, 0.1, staircase=True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    gradients, v = zip(*optimizer.compute_gradients(train_loss))\n",
    "    gradients, _ = tf.clip_by_global_norm(gradients, 1.25)\n",
    "    optimizer = optimizer.apply_gradients(zip(gradients, v), global_step=global_step)\n",
    "\n",
    "    # Validation loop\n",
    "    valid_logits, _ = crnn_model(valid_dataset)\n",
    "\n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(train_logits)\n",
    "    valid_prediction = tf.nn.softmax(valid_logits)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
